{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ9EZXXfiN6j"
   },
   "source": [
    "# MNIST\n",
    "\n",
    "* Entropia Global das Imagens (Entropia Aplicada Diretamente flatten)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy6lG5fwzonZ"
   },
   "source": [
    "# Apartir daqui aplicamos entropia no filtro de validação também (o v09) não executa essa ação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPhyGa9t-s5Y"
   },
   "source": [
    "## Criação e definição de todas as funções\n",
    "\n",
    "1.   Filtro de Entropia\n",
    "2.   Carregamento, Preprocesssamento e Tratamento\n",
    "3.   Criação do Modelo\n",
    "4.   Predições Estatisticas\n",
    "5.   Plot de Acuracy e Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLGOTl6X1wcb"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import time\n",
    "\n",
    "def filtrar_entropia_median(train_X, train_y):  # filtro de entropia\n",
    "    base = 2 # base de logaritimo\n",
    "    tuplasEntropia = [(index, entropy(img.flatten(), base = base)) for index, img in enumerate(train_X)] #operacao de flaten sob base para calcular entropia e enumerar\n",
    "    entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
    "    n = len(entropiesLocal_ordenado)\n",
    "    if n % 2 == 1:\n",
    "        median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
    "    else:\n",
    "        median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
    "    indices_filtrados = [item[0] for item in entropiesLocal_ordenado if item[1] <= median]  #selecionando os indices de entropia entropia baixa abaixo da mediana.\n",
    "    train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
    "    train_y = np.array([train_y[i] for i in indices_filtrados]) # passa os indices selecionados para base de labels\n",
    "    return train_X, train_y\n",
    "\n",
    "def filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y):  # filtro de entropia\n",
    "    base = 2 # base de logaritimo\n",
    "    tuplasEntropia = [(index, entropy(img.flatten(), base = base)) for index, img in enumerate(train_X)] #operacao de flaten sob base para calcular entropia e enumerar\n",
    "    tuplasEntropiaTeste = [(index, entropy(img.flatten(), base = base)) for index, img in enumerate(test_X)] #operacao de flaten sob base para calcular entropia e enumerar\n",
    "    entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
    "    entropiesLocal_ordenadoTeste = sorted(tuplasEntropiaTeste, key=lambda x: x[1]) #ordenação com base na entropia\n",
    "    n = len(entropiesLocal_ordenado)\n",
    "    if n % 2 == 1:\n",
    "        median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
    "    else:\n",
    "        median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
    "    indices_filtrados = [item[0] for item in entropiesLocal_ordenado if item[1] <= median]  #selecionando os indices de entropia entropia baixa abaixo da mediana.\n",
    "    indices_filtradosTeste = [item[0] for item in entropiesLocal_ordenadoTeste if item[1] <= median]  #selecionando os indices de entropia entropia baixa abaixo da mediana.\n",
    "    train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
    "    train_y = np.array([train_y[i] for i in indices_filtrados]) # passa os indices selecionados para base de labels\n",
    "    test_X = np.array([test_X[i] for i in indices_filtradosTeste]) # passa os indices selecionados para base de treino\n",
    "    test_y = np.array([test_y[i] for i in indices_filtradosTeste]) # passa os indices selecionados para base de labels    \n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "def carregarPeparar(train_X, train_y, test_X, test_y, d, c): # Carregamento e Tratamento\n",
    "  train_X = train_X.reshape((train_X.shape[0], d, d, c))   # Ajuste da dimensão\n",
    "  test_X = test_X.reshape((test_X.shape[0], d, d, c))    # Ajuste da dimensão\n",
    "  train_X = train_X.astype(\"float\")/255.0     # Normalização\n",
    "  test_X = test_X.astype(\"float\")/255.0     # Normalização\n",
    "  train_y = to_categorical(train_y, 10) #10 classes possiveis   -   # to Categorical para as classes\n",
    "  test_y = to_categorical(test_y, 10) #10 classes possiveis     -   # to Categorical para as classes\n",
    "  return train_X, train_y, test_X, test_y\n",
    "\n",
    "def avaliacao_statistica(test_X, test_y):\n",
    "    predictions = model.predict(test_X) # Previsões\n",
    "    classePredita = np.argmax(predictions, axis=1)\n",
    "    classeVerdadeira = np.argmax(test_y, axis=1)\n",
    "    cm = confusion_matrix(classeVerdadeira, classePredita)\n",
    "    ConfusionMatrixDisplay(cm).plot()  # classePredita para matrix de confusao\n",
    "    acc = accuracy_score(classeVerdadeira, classePredita) * 100  # Acuracia\n",
    "    print(\"Acuracia: \", '%.3f' % (acc*1.0))\n",
    "    sens = recall_score(classeVerdadeira, classePredita, average='macro')  # Sensibilidade\n",
    "    print(\"Sensibilidade: \", '%.3f' % (sens*1.0))\n",
    "\n",
    "def plotAcuraciaLoss(history): #plots\n",
    "  plt.figure(figsize=(12, 5))\n",
    "  plt.subplot(1, 2, 1)  # 1 linha, 2 colunas, primeiro gráfico  # Primeiro gráfico (accuracy)\n",
    "  plt.plot(history.history['accuracy'])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.title('Model Accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "  plt.subplot(1, 2, 2)  # 1 linha, 2 colunas, segundo gráfico # Segundo gráfico (loss)\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model Loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "  plt.tight_layout() # Exibir os gráficos\n",
    "\n",
    "def plotarDadosTrain(index_inicio, qtd, train_X): # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
    "  base = 330\n",
    "  import matplotlib.pyplot as plt\n",
    "  for i in range(index_inicio, index_inicio + qtd):\n",
    "      plt.subplot(base + 1 + (i - index_inicio))\n",
    "      plt.imshow(train_X[i], cmap=plt.get_cmap('gray'))\n",
    "  plt.show()\n",
    "  print(train_y[index_inicio:index_inicio + qtd])\n",
    "\n",
    "def tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF = None): # x = time.time() || inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
    "  elapsed_time = [[\"Carregar DataSet\", 0], [\"Filtro Entropia\", 0], [\"Pre-processamento\", 0], [\"Criação do Modelo\", 0], [\"Treinamento\", 0], [\"Inicio à Fim Execução\" , 0]]\n",
    "  elapsed_time[0][1] = importacaoF - inicio  # tempo de inicio de execução até o final da importação do dataset\n",
    "  if (entropiaF != None): elapsed_time[1][1] = entropiaF - importacaoF  # Tempo gasto Inicio é importaçãoF - Execução da Entropia\n",
    "  elapsed_time[2][1] = padrozinacaoF - importacaoF  # pega o tempo da entropia e subtrai do tempo apos a padronização para verificar quanto tempo padronizacao demorou\n",
    "  elapsed_time[3][1] = criacaoModeloF - padrozinacaoF  # tempo da padronizacao - tempo apos a criacao do modelo para verificar tempo decorido\n",
    "  elapsed_time[4][1] = treinamentoF - criacaoModeloF  # tempo inicial é marcado pela criacaoModeloF - tempoTreinamento que marca o momento que treinamento terminou\n",
    "  elapsed_time[5][1] = treinamentoF - inicio #tempo total de execução inicio| importações até final da Execução | treinamento\n",
    "  for index, tempo in enumerate(elapsed_time):\n",
    "    if entropiaF is None and index == 1:\n",
    "      continue\n",
    "    print(\"{}: {:.4f}\".format(tempo[0], round(tempo[1], 2)) + str(\" em milissegundos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcmP_31ayQV6"
   },
   "source": [
    "### [MNIST]Treinamento e ordem de execução das funções\n",
    "  * **[Modelo da CNN para MNIST]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkzM7M9RugW0"
   },
   "outputs": [],
   "source": [
    "def criacaoModeloF1(): # Criação do Modelo\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3,3), activation = 'relu', kernel_initializer = 'he_uniform', input_shape = (28, 28, 1)))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "  model.add(Dropout(0.4))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(10, activation = 'softmax'))  # classificação 10 categorias\n",
    "\n",
    "  opt = SGD(learning_rate=0.01, momentum =0.9) #copilaçãoModelo\n",
    "  model.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtJvfWG7D7N-"
   },
   "source": [
    "### [MNIST]Treinamento e ordem de execução das funções\n",
    "  * **[Sem seleção de entropia]**\n",
    "  * **[Dataset Completo]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5l0u7wQDD63z",
    "outputId": "2db6da68-9c24-4a69-a711-bbce700d845f"
   },
   "outputs": [],
   "source": [
    "inicio = time.time() #tempo de inicio\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "importacaoF = time.time() #tempo de importacao\n",
    "train_X, train_y, test_X, test_y = carregarPeparar(train_X, train_y, test_X, test_y, 28, 1) #padronização e categorização\n",
    "padrozinacaoF = time.time() #tempo de padronizacao\n",
    "model = criacaoModeloF1() # instanciando o modelo\n",
    "criacaoModeloF = time.time() #tempo de criacao do modelo\n",
    "history = model.fit(train_X, train_y, epochs = 10, batch_size = 32, validation_data=(test_X, test_y), verbose = 1) #treinamento\n",
    "treinamentoF = time.time() #tempo final\n",
    "\n",
    "tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF) #inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
    "#plotarDadosTrain(3000, 9, train_X) # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
    "avaliacao_statistica(test_X, test_y)\n",
    "plotAcuraciaLoss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGDUPLMzNFb5"
   },
   "source": [
    "### [MNIST]Treinamento e ordem de execução das funções\n",
    "  * **[Sem seleção de entropia]**\n",
    "  * **[Dataset Divido na metade e aletatorio]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Yp2ZxIRnNN0X",
    "outputId": "fbb01c00-1646-48b9-ced7-7e1434000052"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "def selecionar_metade_aleatoria(metade1, metade2):\n",
    "    return metade1 if random.choice([True, False]) else metade2\n",
    "\n",
    "inicio = time.time() #tempo de inicio\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "metade_train_X, outra_metade_train_X, metade_train_y, outra_metade_train_y = train_test_split(train_X, train_y, test_size=0.5, random_state=42) # Corte aleatorio do dataset na metade\n",
    "train_X = selecionar_metade_aleatoria(metade_train_X, outra_metade_train_X)\n",
    "train_y = selecionar_metade_aleatoria(metade_train_y, outra_metade_train_y)\n",
    "\n",
    "importacaoF = time.time() #tempo de importacao\n",
    "train_X, train_y, test_X, test_y = carregarPeparar(train_X, train_y, test_X, test_y, 28, 1) #padronização e categorização\n",
    "padrozinacaoF = time.time() #tempo de padronizacao\n",
    "model = criacaoModeloF1() # instanciando o modelo\n",
    "criacaoModeloF = time.time() #tempo de criacao do modelo\n",
    "history = model.fit(train_X, train_y, epochs = 10, batch_size = 32, validation_data=(test_X, test_y), verbose = 1) #treinamento\n",
    "treinamentoF = time.time() #tempo final\n",
    "\n",
    "tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF) #inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
    "#plotarDadosTrain(3000, 9, train_X) # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
    "avaliacao_statistica(test_X, test_y)\n",
    "plotAcuraciaLoss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NywAshXjCPc6"
   },
   "source": [
    "## [MNIST]Treinamento e ordem de execução das funções\n",
    " * **[Modelo selecionado com entropia]**\n",
    " * **Baixa Entropia**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgYBuah9-0Kt"
   },
   "source": [
    "### Filtro de entropia :\n",
    "  * Dataset filtrado\n",
    "  * Baixa Entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "orD6RONz-xh9",
    "outputId": "44253a27-3f17-4df4-b607-760a45c71940"
   },
   "outputs": [],
   "source": [
    "inicio = time.time() #tempo de inicio\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "importacaoF = time.time() #tempo de importacao\n",
    "train_X, train_y = filtrar_entropia_median(train_X, train_y)  #filtro de entropia sob conjunto de dados de treino\n",
    "entropiaF = time.time() #tempo de filtragem entropia\n",
    "train_X, train_y, test_X, test_y = carregarPeparar(train_X, train_y, test_X, test_y, 28, 1) #padronização e categorização\n",
    "padrozinacaoF = time.time() #tempo de padronizacao\n",
    "model = criacaoModeloF1() # instanciando o modelo\n",
    "criacaoModeloF = time.time() #tempo de criacao do modelo\n",
    "history = model.fit(train_X, train_y, epochs = 10, batch_size = 32, validation_data=(test_X, test_y), verbose = 1) #treinamento\n",
    "treinamentoF = time.time() #tempo final\n",
    "\n",
    "tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF) #inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
    "#plotarDadosTrain(3000, 9, train_X) # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
    "avaliacao_statistica(test_X, test_y)\n",
    "plotAcuraciaLoss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSelnrdf-qu6"
   },
   "source": [
    "### Filtro de entropia :\n",
    "  * Dataset filtrado\n",
    "  * Median transferida para conjunto de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inicio = time.time() #tempo de inicio\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "importacaoF = time.time() #tempo de importacao\n",
    "train_X, train_y, test_X, test_y = filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y) #novo filtro da reunião 03-10-2023\n",
    "entropiaF = time.time() #tempo de filtragem entropia\n",
    "train_X, train_y, test_X, test_y = carregarPeparar(train_X, train_y, test_X, test_y, 28, 1) #padronização e categorização\n",
    "padrozinacaoF = time.time() #tempo de padronizacao\n",
    "model = criacaoModeloF1() # instanciando o modelo\n",
    "criacaoModeloF = time.time() #tempo de criacao do modelo\n",
    "history = model.fit(train_X, train_y, epochs = 10, batch_size = 32, validation_data=(test_X, test_y), verbose = 1) #treinamento\n",
    "treinamentoF = time.time() #tempo final\n",
    "\n",
    "tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF) #inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
    "#plotarDadosTrain(3000, 9, train_X) # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
    "avaliacao_statistica(test_X, test_y)\n",
    "plotAcuraciaLoss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtro de entropia :\n",
    "  * Dataset filtrado\n",
    "  * Median transferida para conjunto de treino\n",
    "  * Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def crossValidation(combinado_treino_X, combinado_validacao_y,n_folds = 5):\n",
    "    #divide seus dados em k folds, e treina no k−1 e valida em 1 fold, então a proporção de dados usados para validação é 1/k.\n",
    "    #2 folds (50% de validação): Treina em 50% dos dados e valida nos outros 50%.\n",
    "    #5 folds (20% de validação): Treina em 80% dos dados e valida em 20%.\n",
    "    #10 folds (10% de validação): Treina em 90% dos dados e valida em 10%.    \n",
    "    n_fold = 5 if n_folds == 0 else n_folds\n",
    "    \n",
    "    kf = KFold(n_splits = n_folds, shuffle = True, random_state = 42)  #divisão do conjunto de dados\n",
    "    fold_no = 1\n",
    "    for train, test in kf.split(combinado_treino_X, combinado_validacao_y):\n",
    "        print(f\"Treinando a rede no folder numero: {fold_no}\")\n",
    "        model = criacaoModeloF1() # instanciando o modelo\n",
    "        inicio = time.time() #tempo de inicio\n",
    "        history = model.fit(combinado_treino_X[train], combinado_validacao_y[train], epochs = 10, batch_size = 32, verbose = 1, validation_data = (combinado_treino_X[test], combinado_validacao_y[test]))\n",
    "        treinamentoF = time.time() #tempo final\n",
    "        tempoTreino = round(treinamentoF - inicio,2)\n",
    "        scores = model.evaluate(combinado_treino_X[test], combinado_validacao_y[test], verbose = 1)\n",
    "        print(f\"Num do Folder: {fold_no}: Taxa de Loss: {scores[0]}, Taxa de Acuracia: {scores[1]*100:.2f}%, Tamanho do Conjunto de Dados: {len(combinado_treino_X[train])}, Tempo de Treino: {tempoTreino}\")\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)  # 1 linha, 2 colunas, primeiro gráfico  # Primeiro gráfico (accuracy)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy']) # Métrica de validação\n",
    "        plt.title(f'Model Accuracy: {fold_no}')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.subplot(1, 2, 2)  # 1 linha, 2 colunas, segundo gráfico # Segundo gráfico (loss)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss']) # Métrica de validação\n",
    "        plt.title(f'Model Loss: {fold_no}')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.tight_layout() # Exibir os gráficos       \n",
    "        fold_no += 1\n",
    "        print(\"\\n\")\n",
    "        \n",
    "#carregar os dados    \n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "train_X, train_y, test_X, test_y = filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y) #novo filtro da reunião 03-10-2023\n",
    "train_X, train_y, test_X, test_y = carregarPeparar(train_X, train_y, test_X, test_y, 28, 1) #padronização e categorização\n",
    "combinado_treino_X = np.concatenate([train_X, test_X], axis=0)\n",
    "combinado_validacao_y = np.concatenate([train_y, test_y], axis=0)\n",
    "\n",
    "#criar/instanciar o modelo para validação cruzada\n",
    "crossValidation(combinado_treino_X,combinado_validacao_y, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGBSsNe4CxBg"
   },
   "source": [
    "# Importando Dados CIFAR-10\n",
    "\n",
    "* Entropia Global das Imagens (Entropia Aplicada Diretamente flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96LR7YwNCxBp"
   },
   "source": [
    "## Criação e definição de todas as funções\n",
    "\n",
    "1.   Filtro de Entropia\n",
    "2.   Carregamento, Preprocesssamento e Tratamento\n",
    "3.   Criação do Modelo [mesmo modelo do MNIST]\n",
    "4.   Predições Estatisticas\n",
    "5.   Plot de Acuracy e Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "def criacaoModeloG9():\n",
    "    opt=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='elu', input_shape=(32, 32, 3), padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(32, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))   \n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(128, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))   \n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-px8VA8DFIx"
   },
   "source": [
    "### [CIFAR]Treinamento e ordem de execução das funções\n",
    " * **[Modelo selecionado sem entropia]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def criacaoModeloG9():\n",
    "    opt=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='elu', input_shape=(32, 32, 3), padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(32, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))   \n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(128, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))   \n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)\n",
    "    return model\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "train_X, train_y, test_X, test_y = carregarPeparar(train_X, train_y, test_X, test_y, 32, 3) #padronização e categorização\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
    "datagen.fit(train_X)\n",
    "    \n",
    "model = criacaoModeloG9()\n",
    "inicio = time.time() #tempo de inicio\n",
    "history=model.fit(datagen.flow(train_X, train_y, batch_size=128),\n",
    "                    steps_per_epoch = len(train_X) / 128, epochs=50, validation_data=(test_X, test_y))\n",
    "treinamentoF = time.time() #tempo final\n",
    "tempoTreino = round(treinamentoF - inicio,2)\n",
    "print(\"Tempo de treinamento: \", tempoTreino)\n",
    "avaliacao_statistica(test_X, test_y)\n",
    "plotAcuraciaLoss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bvdvgxeDFsC"
   },
   "source": [
    "  ### [CIFAR]Treinamento e ordem de execução das funções\n",
    "  * **[Sem seleção de entropia]**\n",
    "  * **[Dataset Divido na metade e aletatorio]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tdQcWiZyDF4F",
    "outputId": "a3dec068-fc65-433e-d2e2-75c42b2fb32c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "def selecionar_metade_aleatoria(metade1, metade2):\n",
    "    return metade1 if random.choice([True, False]) else metade2\n",
    "\n",
    "inicio = time.time() #tempo de inicio\n",
    "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "metade_train_X, outra_metade_train_X, metade_train_y, outra_metade_train_y = train_test_split(train_X, train_y, test_size=0.5, random_state=42) # Corte aleatorio do dataset na metade\n",
    "train_X = selecionar_metade_aleatoria(metade_train_X, outra_metade_train_X)\n",
    "train_y = selecionar_metade_aleatoria(metade_train_y, outra_metade_train_y)\n",
    "\n",
    "importacaoF = time.time() #tempo de importacao\n",
    "train_X, train_y, test_X, test_y = carregarPeparar(train_X, train_y, test_X, test_y, 32, 3) #padronização e categorização\n",
    "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
    "datagen.fit(train_X)\n",
    "padrozinacaoF = time.time() #tempo de padronizacao\n",
    "model = criacaoModeloG9()\n",
    "criacaoModeloF = time.time() #tempo de criacao do modelo\n",
    "history=model.fit(datagen.flow(train_X, train_y, batch_size=128),\n",
    "                    steps_per_epoch = len(train_X) / 128, epochs=50, validation_data=(test_X, test_y))\n",
    "treinamentoF = time.time() #tempo final\n",
    "\n",
    "tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF) #inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
    "#plotarDadosTrain(3000, 9, train_X) # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
    "avaliacao_statistica(test_X, test_y)\n",
    "plotAcuraciaLoss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIOmmoBiDSL8"
   },
   "source": [
    "### [CIFAR]Treinamento e ordem de execução das funções\n",
    " * **[Modelo selecionado com entropia]**\n",
    " * **[Baixa entropia]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ruUim6kwC9XQ",
    "outputId": "4c11e3db-2687-4c14-a697-acd59f358b27"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "inicio = time.time() #tempo de inicio\n",
    "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "importacaoF = time.time() #tempo de importacao\n",
    "train_X, train_y = filtrar_entropia_median(train_X, train_y)  #filtro de entropia sob conjunto de dados\n",
    "entropiaF = time.time() #tempo de filtragem entropia\n",
    "train_X, train_y, test_X, test_y = carregarPeparar(train_X, train_y, test_X, test_y, 32, 3) #padronização e categorização\n",
    "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
    "datagen.fit(train_X)\n",
    "padrozinacaoF = time.time() #tempo de padronizacao\n",
    "model = criacaoModeloG9() # instanciando o modelo\n",
    "criacaoModeloF = time.time() #tempo de criacao do modelo\n",
    "history=model.fit(datagen.flow(train_X, train_y, batch_size=128),\n",
    "                    steps_per_epoch = len(train_X) / 128, epochs=50, validation_data=(test_X, test_y))\n",
    "treinamentoF = time.time() #tempo final\n",
    "\n",
    "tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF) #inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
    "#plotarDadosTrain(3000, 9, train_X) # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
    "avaliacao_statistica(test_X, test_y)\n",
    "plotAcuraciaLoss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfb0j0jZqbY-"
   },
   "source": [
    "### Filtro de entropia :\n",
    "  * Dataset filtrado\n",
    "  * Median transferida para conjunto de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "inicio = time.time() #tempo de inicio\n",
    "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "importacaoF = time.time() #tempo de importacao\n",
    "train_X, train_y, test_X, test_y = filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y) #novo filtro da reunião 03-10-2023\n",
    "entropiaF = time.time() #tempo de filtragem entropia\n",
    "train_X, train_y, test_X, test_y = carregarPeparar(train_X, train_y, test_X, test_y, 32, 3) #padronização e categorização\n",
    "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
    "datagen.fit(train_X)\n",
    "padrozinacaoF = time.time() #tempo de padronizacao\n",
    "model = criacaoModeloG9() # instanciando o modelo\n",
    "criacaoModeloF = time.time() #tempo de criacao do modelo\n",
    "history=model.fit(datagen.flow(train_X, train_y, batch_size=128),\n",
    "                    steps_per_epoch = len(train_X) / 128, epochs=50, validation_data=(test_X, test_y))\n",
    "treinamentoF = time.time() #tempo final\n",
    "\n",
    "tempDecorrido(inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF) #inicio, importacaoF, padrozinacaoF, criacaoModeloF, treinamentoF, entropiaF\n",
    "#plotarDadosTrain(3000, 9, train_X) # Exemplo: apartir do indice N, exibe K imagens do conjunto de dados, D\n",
    "avaliacao_statistica(test_X, test_y)\n",
    "plotAcuraciaLoss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Filtro de entropia :\n",
    "  * Dataset filtrado\n",
    "  * Median transferida para conjunto de treino\n",
    "  * Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def crossValidation(combinado_treino_X, combinado_validacao_y,n_folds = 5):\n",
    "    #divide seus dados em k folds, e treina no k−1 e valida em 1 fold, então a proporção de dados usados para validação é 1/k.\n",
    "    #2 folds (50% de validação): Treina em 50% dos dados e valida nos outros 50%.\n",
    "    #5 folds (20% de validação): Treina em 80% dos dados e valida em 20%.\n",
    "    #10 folds (10% de validação): Treina em 90% dos dados e valida em 10%.    \n",
    "    n_fold = 5 if n_folds == 0 else n_folds\n",
    "    datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
    "    datagen.fit(combinado_treino_X)\n",
    "    \n",
    "    kf = KFold(n_splits = n_folds, shuffle = True, random_state = 42)  #divisão do conjunto de dados\n",
    "    fold_no = 1\n",
    "    for train, test in kf.split(combinado_treino_X, combinado_validacao_y):\n",
    "        print(f\"Treinando a rede no folder numero: {fold_no}\")\n",
    "        model = criacaoModeloG9() # instanciando o modelo\n",
    "        inicio = time.time() #tempo de inicio\n",
    "        history=model.fit(datagen.flow(combinado_treino_X[train], combinado_validacao_y[train], batch_size=128),\n",
    "                            steps_per_epoch = len(combinado_treino_X[train]) / 128, epochs=50, validation_data=(combinado_treino_X[test], combinado_validacao_y[test]))        \n",
    "           \n",
    "        treinamentoF = time.time() #tempo final\n",
    "        tempoTreino = round(treinamentoF - inicio,2)\n",
    "        scores = model.evaluate(combinado_treino_X[test], combinado_validacao_y[test], verbose = 1)\n",
    "        print(f\"Num do Folder: {fold_no}: Taxa de Loss: {scores[0]}, Taxa de Acuracia: {scores[1]*100:.2f}%, Tamanho do Conjunto de Dados: {len(combinado_treino_X[train])}, Tempo de Treino: {tempoTreino}\")\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)  # 1 linha, 2 colunas, primeiro gráfico  # Primeiro gráfico (accuracy)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy']) # Métrica de validação\n",
    "        plt.title(f'Model Accuracy: {fold_no}')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.subplot(1, 2, 2)  # 1 linha, 2 colunas, segundo gráfico # Segundo gráfico (loss)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss']) # Métrica de validação\n",
    "        plt.title(f'Model Loss: {fold_no}')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.tight_layout() # Exibir os gráficos       \n",
    "        fold_no += 1\n",
    "        print(\"\\n\")\n",
    "        \n",
    "#carregar os dados\n",
    "from keras.datasets import cifar10\n",
    "(train_X, train_y), (test_X, test_y) = cifar10.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "train_X, train_y, test_X, test_y = filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y) #novo filtro da reunião 03-10-2023\n",
    "train_X, train_y, test_X, test_y = carregarPeparar(train_X, train_y, test_X, test_y, 32, 3) #carregar preparar\n",
    "combinado_treino_X = np.concatenate([train_X, test_X], axis=0)\n",
    "combinado_validacao_y = np.concatenate([train_y, test_y], axis=0)\n",
    "\n",
    "#criar/instanciar o modelo para validação cruzada\n",
    "crossValidation(combinado_treino_X,combinado_validacao_y, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Cifar100]Treinamento e ordem de execução das funções\n",
    "  * **[Sem seleção de entropia]**\n",
    "  * **[Dataset Completo]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, UpSampling2D, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from keras.datasets import mnist\n",
    "from scipy.stats import entropy\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from PIL import Image\n",
    "import time\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "def plotAcuraciaLoss(history): #plots\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1 linha, 2 colunas, primeiro gráfico  # Primeiro gráfico (accuracy)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.subplot(1, 2, 2)  # 1 linha, 2 colunas, segundo gráfico # Segundo gráfico (loss)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.tight_layout() # Exibir os gráficos\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "train_y = to_categorical(train_y, num_classes = 100)\n",
    "test_y = to_categorical(test_y, num_classes = 100)\n",
    "train_X = train_X.astype(\"float\")/255\n",
    "test_X = test_X.astype(\"float\")/255\n",
    "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
    "datagen.fit(train_X)\n",
    "\n",
    "#Importing the Resnet Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "resnet_model = ResNet50(\n",
    "    include_top = False,\n",
    "    weights = 'imagenet',\n",
    "    input_shape = (224,224,3)\n",
    ")\n",
    "for layer in resnet_model.layers:\n",
    "    if isinstance(layer, BatchNormalization):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "#reducao da taxa de aprendizagem\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    factor=0.96,\n",
    "    min_lr=1e-8)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(UpSampling2D(size=(7, 7),interpolation='bilinear'))\n",
    "model.add(resnet_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=128),\n",
    "    steps_per_epoch=len(train_X) / 128,\n",
    "    epochs=50,\n",
    "    validation_data=(test_X, test_y),\n",
    "    callbacks=[learning_rate_reduction]\n",
    ")\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "print(\"train_accuracy: \", train_accuracy)\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "print(\"val_accuracy: \", val_accuracy)\n",
    "plotAcuraciaLoss(history)\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "print(\"train_accuracy: \", train_accuracy)\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "print(\"val_accuracy: \", val_accuracy)\n",
    "final_train_accuracy = train_accuracy[-1]\n",
    "final_val_accuracy = val_accuracy[-1]\n",
    "print(\"Final training accuracy: {:.2f}%\".format(final_train_accuracy * 100))\n",
    "print(\"Final validation accuracy: {:.2f}%\".format(final_val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Cifar100]Treinamento e ordem de execução das funções\n",
    "  * **[Sem seleção de entropia]**\n",
    "  * **[Dataset Divido na metade e aletatorio]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, UpSampling2D, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from keras.datasets import mnist\n",
    "from scipy.stats import entropy\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from PIL import Image\n",
    "import time\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "def plotAcuraciaLoss(history): #plots\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1 linha, 2 colunas, primeiro gráfico  # Primeiro gráfico (accuracy)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.subplot(1, 2, 2)  # 1 linha, 2 colunas, segundo gráfico # Segundo gráfico (loss)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.tight_layout() # Exibir os gráficos\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "def selecionar_metade_aleatoria(metade1, metade2):\n",
    "    return metade1 if random.choice([True, False]) else metade2\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "metade_train_X, outra_metade_train_X, metade_train_y, outra_metade_train_y = train_test_split(train_X, train_y, test_size=0.5, random_state=42) # Corte aleatorio do dataset na metade\n",
    "train_X = selecionar_metade_aleatoria(metade_train_X, outra_metade_train_X)\n",
    "train_y = selecionar_metade_aleatoria(metade_train_y, outra_metade_train_y)\n",
    "\n",
    "train_y = to_categorical(train_y, num_classes = 100)\n",
    "test_y = to_categorical(test_y, num_classes = 100)\n",
    "train_X = train_X.astype(\"float\")/255\n",
    "test_X = test_X.astype(\"float\")/255\n",
    "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
    "datagen.fit(train_X)\n",
    "\n",
    "#Importing the Resnet Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "resnet_model = ResNet50(\n",
    "    include_top = False,\n",
    "    weights = 'imagenet',\n",
    "    input_shape = (224,224,3)\n",
    ")\n",
    "for layer in resnet_model.layers:\n",
    "    if isinstance(layer, BatchNormalization):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "#reducao da taxa de aprendizagem\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    factor=0.96,\n",
    "    min_lr=1e-8)\n",
    "# Adicionando a arquitetura do modelo\n",
    "model = Sequential()\n",
    "model.add(UpSampling2D(size=(7, 7),interpolation='bilinear'))\n",
    "model.add(resnet_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=128),\n",
    "    steps_per_epoch=len(train_X) / 128,\n",
    "    epochs=50,\n",
    "    validation_data=(test_X, test_y),\n",
    "    callbacks=[learning_rate_reduction]\n",
    ")\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "print(\"train_accuracy: \", train_accuracy)\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "print(\"val_accuracy: \", val_accuracy)\n",
    "plotAcuraciaLoss(history)\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "print(\"train_accuracy: \", train_accuracy)\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "print(\"val_accuracy: \", val_accuracy)\n",
    "final_train_accuracy = train_accuracy[-1]\n",
    "final_val_accuracy = val_accuracy[-1]\n",
    "print(\"Final training accuracy: {:.2f}%\".format(final_train_accuracy * 100))\n",
    "print(\"Final validation accuracy: {:.2f}%\".format(final_val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Cifar100]Filtro de entropia :\n",
    "  * Dataset filtrado\n",
    "  * Baixa Entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, UpSampling2D, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from keras.datasets import mnist\n",
    "from scipy.stats import entropy\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from PIL import Image\n",
    "import time\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "def filtrar_entropia_median(train_X, train_y):  # filtro de entropia\n",
    "    base = 2 # base de logaritimo\n",
    "    tuplasEntropia = [(index, entropy(img.flatten(), base = base)) for index, img in enumerate(train_X)] #operacao de flaten sob base para calcular entropia e enumerar\n",
    "    entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
    "    n = len(entropiesLocal_ordenado)\n",
    "    if n % 2 == 1:\n",
    "        median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
    "    else:\n",
    "        median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
    "    indices_filtrados = [item[0] for item in entropiesLocal_ordenado if item[1] <= median]  #selecionando os indices de entropia entropia baixa abaixo da mediana.\n",
    "    train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
    "    train_y = np.array([train_y[i] for i in indices_filtrados]) # passa os indices selecionados para base de labels\n",
    "    return train_X, train_y\n",
    "\n",
    "def plotAcuraciaLoss(history): #plots\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1 linha, 2 colunas, primeiro gráfico  # Primeiro gráfico (accuracy)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.subplot(1, 2, 2)  # 1 linha, 2 colunas, segundo gráfico # Segundo gráfico (loss)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.tight_layout() # Exibir os gráficos\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "tempoFiltro1 = time.time() #tempo de filtro\n",
    "train_X, train_y = filtrar_entropia_median(train_X,train_y)\n",
    "tempoFiltro2 = time.time() #tempo de filtro\n",
    "\n",
    "tempoFinalFiltro = tempoFiltro2 - tempoFiltro1\n",
    "\n",
    "train_y = to_categorical(train_y, num_classes = 100)\n",
    "test_y = to_categorical(test_y, num_classes = 100)\n",
    "train_X = train_X.astype(\"float\")/255\n",
    "test_X = test_X.astype(\"float\")/255\n",
    "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
    "datagen.fit(train_X)\n",
    "\n",
    "#Importing the Resnet Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "resnet_model = ResNet50(\n",
    "    include_top = False,\n",
    "    weights = 'imagenet',\n",
    "    input_shape = (224,224,3)\n",
    ")\n",
    "for layer in resnet_model.layers:\n",
    "    if isinstance(layer, BatchNormalization):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "#reducao da taxa de aprendizagem\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    factor=0.96,\n",
    "    min_lr=1e-8)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(UpSampling2D(size=(7, 7),interpolation='bilinear'))\n",
    "model.add(resnet_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "tempoTreino1 = time.time() #tempo de treino\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=128),\n",
    "    steps_per_epoch=len(train_X) / 128,\n",
    "    epochs=50,\n",
    "    validation_data=(test_X, test_y),\n",
    "    callbacks=[learning_rate_reduction]\n",
    ")\n",
    "tempoTreino2 = time.time() #tempo de treino\n",
    "\n",
    "tempoTreinoFinal = tempoTreino2 - tempoTreino1\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "print(\"train_accuracy: \", train_accuracy)\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "print(\"val_accuracy: \", val_accuracy)\n",
    "plotAcuraciaLoss(history)\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "print(\"train_accuracy: \", train_accuracy)\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "print(\"val_accuracy: \", val_accuracy)\n",
    "final_train_accuracy = train_accuracy[-1]\n",
    "final_val_accuracy = val_accuracy[-1]\n",
    "print(\"Final training accuracy: {:.2f}%\".format(final_train_accuracy * 100))\n",
    "print(\"Final validation accuracy: {:.2f}%\".format(final_val_accuracy * 100))\n",
    "print(\"Tempo usado para filtrar: \", tempoFinalFiltro)\n",
    "print(\"Tempo usado para treinamento: \", tempoTreinoFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Cifar100]Filtro de entropia :\n",
    "  * Dataset filtrado\n",
    "  * Baixa Entropia (nos dados de treino e teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, UpSampling2D, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from keras.datasets import mnist\n",
    "from scipy.stats import entropy\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from PIL import Image\n",
    "import time\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "\n",
    "def filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y):  # filtro de entropia\n",
    "    base = 2 # base de logaritimo\n",
    "    tuplasEntropia = [(index, entropy(img.flatten(), base = base)) for index, img in enumerate(train_X)] #operacao de flaten sob base para calcular entropia e enumerar\n",
    "    tuplasEntropiaTeste = [(index, entropy(img.flatten(), base = base)) for index, img in enumerate(test_X)] #operacao de flaten sob base para calcular entropia e enumerar\n",
    "    entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
    "    entropiesLocal_ordenadoTeste = sorted(tuplasEntropiaTeste, key=lambda x: x[1]) #ordenação com base na entropia\n",
    "    n = len(entropiesLocal_ordenado)\n",
    "    if n % 2 == 1:\n",
    "        median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
    "    else:\n",
    "        median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
    "    indices_filtrados = [item[0] for item in entropiesLocal_ordenado if item[1] <= median]  #selecionando os indices de entropia entropia baixa abaixo da mediana.\n",
    "    indices_filtradosTeste = [item[0] for item in entropiesLocal_ordenadoTeste if item[1] <= median]  #selecionando os indices de entropia entropia baixa abaixo da mediana.\n",
    "    train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
    "    train_y = np.array([train_y[i] for i in indices_filtrados]) # passa os indices selecionados para base de labels\n",
    "    test_X = np.array([test_X[i] for i in indices_filtradosTeste]) # passa os indices selecionados para base de treino\n",
    "    test_y = np.array([test_y[i] for i in indices_filtradosTeste]) # passa os indices selecionados para base de labels\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "def plotAcuraciaLoss(history): #plots\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1 linha, 2 colunas, primeiro gráfico  # Primeiro gráfico (accuracy)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.subplot(1, 2, 2)  # 1 linha, 2 colunas, segundo gráfico # Segundo gráfico (loss)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.tight_layout() # Exibir os gráficos\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "tempoFiltro1 = time.time() #tempo de filtro\n",
    "train_X, train_y, test_X, test_y = filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y) #novo filtro da reunião 03-10-2023\n",
    "tempoFiltro2 = time.time() #tempo de filtro\n",
    "tempoFinalFiltro = tempoFiltro2 - tempoFiltro1\n",
    "\n",
    "train_y = to_categorical(train_y, num_classes = 100)\n",
    "test_y = to_categorical(test_y, num_classes = 100)\n",
    "train_X = train_X.astype(\"float\")/255\n",
    "test_X = test_X.astype(\"float\")/255\n",
    "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
    "datagen.fit(train_X)\n",
    "\n",
    "#Importing the Resnet Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "resnet_model = ResNet50(\n",
    "    include_top = False,\n",
    "    weights = 'imagenet',\n",
    "    input_shape = (224,224,3)\n",
    ")\n",
    "for layer in resnet_model.layers:\n",
    "    if isinstance(layer, BatchNormalization):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "#reducao da taxa de aprendizagem\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    factor=0.96,\n",
    "    min_lr=1e-8)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(UpSampling2D(size=(7, 7),interpolation='bilinear'))\n",
    "model.add(resnet_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "tempoTreino1 = time.time() #tempo de treino\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=128),\n",
    "    steps_per_epoch=len(train_X) / 128,\n",
    "    epochs=50,\n",
    "    validation_data=(test_X, test_y),\n",
    "    callbacks=[learning_rate_reduction]\n",
    ")\n",
    "tempoTreino2 = time.time() #tempo de treino\n",
    "\n",
    "tempoTreinoFinal = tempoTreino2 - tempoTreino1\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "print(\"train_accuracy: \", train_accuracy)\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "print(\"val_accuracy: \", val_accuracy)\n",
    "plotAcuraciaLoss(history)\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "print(\"train_accuracy: \", train_accuracy)\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "print(\"val_accuracy: \", val_accuracy)\n",
    "final_train_accuracy = train_accuracy[-1]\n",
    "final_val_accuracy = val_accuracy[-1]\n",
    "print(\"Final training accuracy: {:.2f}%\".format(final_train_accuracy * 100))\n",
    "print(\"Final validation accuracy: {:.2f}%\".format(final_val_accuracy * 100))\n",
    "print(\"Tempo usado para filtrar: \", tempoFinalFiltro)\n",
    "print(\"Tempo usado para treinamento: \", tempoTreinoFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Cifar100]Filtro de entropia :\n",
    "  * Dataset filtrado\n",
    "  * Median transferida para conjunto de treino\n",
    "  * Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, UpSampling2D, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from keras.datasets import mnist\n",
    "from scipy.stats import entropy\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from PIL import Image\n",
    "import time\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y):  # filtro de entropia\n",
    "    base = 2 # base de logaritimo\n",
    "    tuplasEntropia = [(index, entropy(img.flatten(), base = base)) for index, img in enumerate(train_X)] #operacao de flaten sob base para calcular entropia e enumerar\n",
    "    tuplasEntropiaTeste = [(index, entropy(img.flatten(), base = base)) for index, img in enumerate(test_X)] #operacao de flaten sob base para calcular entropia e enumerar\n",
    "    entropiesLocal_ordenado = sorted(tuplasEntropia, key=lambda x: x[1]) #ordenação com base na entropia\n",
    "    entropiesLocal_ordenadoTeste = sorted(tuplasEntropiaTeste, key=lambda x: x[1]) #ordenação com base na entropia\n",
    "    n = len(entropiesLocal_ordenado)\n",
    "    if n % 2 == 1:\n",
    "        median = entropiesLocal_ordenado[n // 2][1] # mediana elemento do meio - impares\n",
    "    else:\n",
    "        median = (entropiesLocal_ordenado[n // 2 - 1][1] + entropiesLocal_ordenado[n // 2][1]) / 2.0  # mediana elemento do meio-pares\n",
    "    indices_filtrados = [item[0] for item in entropiesLocal_ordenado if item[1] <= median]  #selecionando os indices de entropia entropia baixa abaixo da mediana.\n",
    "    indices_filtradosTeste = [item[0] for item in entropiesLocal_ordenadoTeste if item[1] <= median]  #selecionando os indices de entropia entropia baixa abaixo da mediana.\n",
    "    train_X = np.array([train_X[i] for i in indices_filtrados]) # passa os indices selecionados para base de treino\n",
    "    train_y = np.array([train_y[i] for i in indices_filtrados]) # passa os indices selecionados para base de labels\n",
    "    test_X = np.array([test_X[i] for i in indices_filtradosTeste]) # passa os indices selecionados para base de treino\n",
    "    test_y = np.array([test_y[i] for i in indices_filtradosTeste]) # passa os indices selecionados para base de labels\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "def plotAcuraciaLoss(history): #plots\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1 linha, 2 colunas, primeiro gráfico  # Primeiro gráfico (accuracy)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.subplot(1, 2, 2)  # 1 linha, 2 colunas, segundo gráfico # Segundo gráfico (loss)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.tight_layout() # Exibir os gráficos\n",
    "\n",
    "def CriacaoModeloGC100 ():\n",
    "  resnet_model = ResNet50(\n",
    "      include_top = False,\n",
    "      weights = 'imagenet',\n",
    "      input_shape = (224,224,3)\n",
    "  )\n",
    "  for layer in resnet_model.layers:\n",
    "      if isinstance(layer, BatchNormalization):\n",
    "          layer.trainable = True\n",
    "      else:\n",
    "          layer.trainable = False\n",
    "\n",
    "  learning_rate_reduction = ReduceLROnPlateau(\n",
    "      monitor='val_accuracy',\n",
    "      patience=2,\n",
    "      verbose=1,\n",
    "      factor=0.96,\n",
    "      min_lr=1e-8)\n",
    "  model = Sequential()\n",
    "  model.add(UpSampling2D(size=(7, 7),interpolation='bilinear'))\n",
    "  model.add(resnet_model)\n",
    "  model.add(GlobalAveragePooling2D())\n",
    "  model.add(Dropout(0.25))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=None))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dropout(0.25))\n",
    "  model.add(Dense(100, activation='softmax'))\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n",
    "  model.compile(\n",
    "      optimizer = optimizer,\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy']\n",
    "  )\n",
    "  return model, learning_rate_reduction\n",
    "\n",
    "def crossValidation(combinado_treino_X, combinado_validacao_y,n_folds = 5):\n",
    "    from sklearn.model_selection import KFold\n",
    "    #divide seus dados em k folds, e treina no k−1 e valida em 1 fold, então a proporção de dados usados para validação é 1/k.\n",
    "    #2 folds (50% de validação): Treina em 50% dos dados e valida nos outros 50%.\n",
    "    #5 folds (20% de validação): Treina em 80% dos dados e valida em 20%.\n",
    "    #10 folds (10% de validação): Treina em 90% dos dados e valida em 10%.\n",
    "    n_fold = 5 if n_folds == 0 else n_folds\n",
    "    datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1)\n",
    "    datagen.fit(combinado_treino_X)\n",
    "\n",
    "    #Diretorio para salvar\n",
    "    output_dir = 'cross_validation_outputs'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    kf = KFold(n_splits = n_folds, shuffle = True, random_state = 42)  #divisão do conjunto de dados\n",
    "    fold_no = 1\n",
    "    for train, test in kf.split(combinado_treino_X, combinado_validacao_y):\n",
    "        print(f\"Treinando a rede no folder numero: {fold_no}\")\n",
    "        model, learning_rate_reduction = CriacaoModeloGC100() # instanciando o modelo\n",
    "        tempoTreino1 = time.time() #tempo de treino\n",
    "        history = model.fit(\n",
    "            datagen.flow(combinado_treino_X[train], combinado_validacao_y[train], batch_size=128),\n",
    "            steps_per_epoch=len(combinado_treino_X[train]) / 128,\n",
    "            epochs=50,\n",
    "            validation_data=(combinado_treino_X[test], combinado_validacao_y[test]),\n",
    "            callbacks=[learning_rate_reduction]\n",
    "        )\n",
    "        tempoTreino2 = time.time() #tempo de treino\n",
    "        tempoTreino = round(tempoTreino2 - tempoTreino1,2)\n",
    "        scores = model.evaluate(combinado_treino_X[test], combinado_validacao_y[test], verbose = 1)\n",
    "        print(f\"Num do Folder: {fold_no}: Taxa de Loss: {scores[0]}, Taxa de Acuracia: {scores[1]*100:.2f}%, Tamanho do Conjunto de Dados: {len(combinado_treino_X[train])}, Tempo de Treino: {tempoTreino}\")\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)  # 1 linha, 2 colunas, primeiro gráfico  # Primeiro gráfico (accuracy)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy']) # Métrica de validação\n",
    "        plt.title(f'Model Accuracy: {fold_no}')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.subplot(1, 2, 2)  # 1 linha, 2 colunas, segundo gráfico # Segundo gráfico (loss)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss']) # Métrica de validação\n",
    "        plt.title(f'Model Loss: {fold_no}')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.tight_layout() # Exibir os gráficos\n",
    "\n",
    "        plt.savefig(os.path.join(output_dir, f'fold_{fold_no}_graph.png'))  # Salvar o gráfico\n",
    "        # Salvando os dados de treinamento\n",
    "        with open(os.path.join(output_dir, f'fold_{fold_no}_train_data.pkl'), 'wb') as file:\n",
    "            pickle.dump((combinado_treino_X[train], combinado_validacao_y[train]), file)\n",
    "        fold_no += 1\n",
    "        print(\"\\n\")\n",
    "\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = cifar100.load_data() # Carregar o conjunto de dados # Importar e Treinar\n",
    "\n",
    "tempoFiltro1 = time.time() #tempo de filtro\n",
    "train_X, train_y, test_X, test_y = filtrar_entropia_median_train_teste(train_X, train_y, test_X, test_y) #novo filtro da reunião 03-10-2023\n",
    "tempoFiltro2 = time.time() #tempo de filtro\n",
    "tempoFinalFiltro = tempoFiltro2 - tempoFiltro1\n",
    "\n",
    "train_y = to_categorical(train_y, num_classes = 100)\n",
    "test_y = to_categorical(test_y, num_classes = 100)\n",
    "train_X = train_X.astype(\"float\")/255\n",
    "test_X = test_X.astype(\"float\")/255\n",
    "\n",
    "combinado_treino_X = np.concatenate([train_X, test_X], axis=0)\n",
    "combinado_validacao_y = np.concatenate([train_y, test_y], axis=0)\n",
    "\n",
    "#criar/instanciar o modelo para validação cruzada\n",
    "crossValidation(combinado_treino_X,combinado_validacao_y, 10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
